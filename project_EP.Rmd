---
title: "Applied Regression Final Project"
author: "Elda Piedade, Thomas Oswald"
date: "May 4th, 2020"
header-includes:
- \usepackage{xcolor}
- \usepackage{framed}
output: 
  pdf_document: default
  html_document:
    df_print: paged
   
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



# Introduction


\colorlet{shadecolor}{gray!10}


\begin{shaded}


The objective of this project is to find the best linear regression model to predict the median home value for the houses in the Houston neighborhoods. Data was gathered from 96 zip codes in Houston by utilizing python web scrapping resources to collect data from the Texas Hometown Locator website (owned by HTL, Inc.).With the dataset extracted and cleaned, exploratory data analysis and statistical analysis were performed to understand the relationship between the median home value and other variables, such as diversity index, per capita income, and average household size. Based on the analysis, data was modeled with linear regression. 


\end{shaded}

# Importance

\colorlet{shadecolor}{gray!10}


\begin{shaded}

1. With a good model for prediction and analysis, individuals in Houston will be able to understand how to price their homes for sale.
2. Understanding how the demographic factors relate to median home value is valuable social knowledge.

\end{shaded}
# Data 
\colorlet{shadecolor}{gray!10}


\begin{shaded}
Response variable : Median Home Value

Predictors:

* $x_1$ -Total Population

* $x_2$ - Diversity Index

* $x_3$ - Median Household Income

* $x_4$ - Per Capita Income

* $x_5$ - Total Housing Units

* $x_6$ - Average Household Size

* $x_7$ - Housing affordability Index

\end{shaded}

### Explained variables:

\colorlet{shadecolor}{gray!10}

\begin{shaded}
1. The Diversity Index is a scale of 0 to 100 that represents the
likelihood that two persons, chosen at random from the same area,
belong to different races or ethnic groups. If an area's entire
population belongs to one race and one ethnic group, then the area
has zero diversity. An area's diversity index increases to 100 when
the population is evenly divided into two or more race/ethnic
groups.
2. The Housing Affordability Index base is 100 and represents a
balance point where a resident with a median household income
can normally qualify to purchase a median price home. Values
above 100 indicate increased affordability, while values below
100 indicate decreased affordability.
3. The outcome variable is the median home value ( median price
home).

\end{shaded}

\pagebreak

# Data Loading & Checking full Model Accuracy:

\colorlet{shadecolor}{gray!10}


\begin{shaded}

After loeading the data and creating a full Linear Regression (LR) model, we found that the model is not adequate. The residuals have a tunel and bowl shape; the data is heavily-tailed distribution with three possible influencial points. To address this problem we will inspect the dat and perform a few transformations.

\end{shaded}

### First model - Residual Plots:
```{r, echo= FALSE, out.height= "27%"}
suppressMessages(library(ggfortify))
suppressMessages(library(car))
suppressMessages(library(olsrr))
suppressMessages(library(MPV))
suppressMessages(library(gridExtra))
suppressMessages(library(cvTools))

reduce_dat <- read.csv("CSV/DATA/texas_Gazetteer_numeric.csv")
reduce_dat <- reduce_dat[c("Median.Home.Value","Total.Population","Diversity.Index1","Median.Household.Income",
           "Per.Capita.Income","Total.Housing.Units", "Average.Household.Size","Housing.Affordability.Index2")]

y <- reduce_dat$Median.Home.Value
x1 <- reduce_dat$Total.Population
x2 <- reduce_dat$Diversity.Index1
x3 <- reduce_dat$Median.Household.Income
x4 <- reduce_dat$Per.Capita.Income
x5 <- reduce_dat$Total.Housing.Units
x6 <- reduce_dat$Average.Household.Size
x7 <- reduce_dat$Housing.Affordability.Index2

fit <- lm(y~x1+x2+x3+x4+x5+x6+x7,data=reduce_dat)
autoplot(fit, size = 0.5, colour = 'sienna2')[1:2]
```


# Data Transformation

\colorlet{shadecolor}{gray!10}


\begin{shaded}

Constant variance of erros assumptions can often be solve with response variable transformations. The log transformations performed the best compared to other transformations. It has the most appropriate properties for the normality of residuals and constant variance. The residual plot for the other transformation (reciprocal, square root, reciprocal square root and inverse) did not show much improvement from the original model and were very influenced by possible influencial points.

In conclusion, our best transformation is the " log" transformation. The residual plot does not appear to have any alarming shape, and the the residuals are normally distributed, except for the problematic observations, 9, 49 and 58. Our new transformed model indicate that a linear model provides a decent fit to the data.

\end{shaded}

### Log Transformmed model - Residual Plots:

```{r, echo= FALSE,out.height= "27%"}
y_recs <- y^(-1/2)
y_rec <- y^(-1)
fit1 <- lm(sqrt(y)~x1+x2+x3+x4+x5+x6+x7,data=reduce_dat)
fit2 <- lm(log(y)~x1+x2+x3+x4+x5+x6+x7,data=reduce_dat)
fit3 <- lm(y_recs ~x1+x2+x3+x4+x5+x6+x7,data=reduce_dat)
fit4 <- lm(y_rec~x1+x2+x3+x4+x5+x6+x7,data=reduce_dat)

autoplot(fit2,size = 0.5, colour = 'sienna2')[1:2]

```
\pagebreak

# Full Regression Model Significance Test

\colorlet{shadecolor}{gray!10}


\begin{shaded}

Our linear regression model is significant given that the p-value for the F-test is smaller than our level of significance 0.05. Looking at the regressor individually, we found that the intercept, total population, median household income, total housing units and housing affordability index are signifacnt for predicting log(median home value). In model detail the F-test is perfomed to understand if at least one regressor is not equal to zero. The conclusion is supported by the t-test performed for each regressor.
This is a first good step in our analysis and important to keep in mind.

\end{shaded}

```{r, echo= FALSE}
summary(lm(log(Median.Home.Value) ~., data = reduce_dat))
```

# Evaluating all possible subset regression models

\colorlet{shadecolor}{gray!10}


\begin{shaded}

In looking for the "best" model, certain criteria must be met inorder for proper variable selection of the regressor equation.  These criteria help us to be able to explain the data in the simpliest way with redundant predictors removed inorder minimize cost and to avoid multi-collinearity in our regression model. 

The criteria for our variable selection include:
1) Large $R^2$ value
2) Maximum Adjusted $R^2$ value
3) Minimum MSres
4) Minimum Mallow's $C_p$ Statistic value.


Based on the above criteria, the "best" candidate models are:

1) Model 1: 

$log(y^{hat})= (1.136e+01) + (2.438e-05)x_4$

2) Model 8:	

$log(y^{hat})=(1.232e+01) +  (1.409e-05)x_3 -(7.258e-03 )x_7$

3) Model 29: 

$log(y^{hat})=(1.267e+01) +(1.301e-05)x_3   -(1.547e-01)x_6  -(6.197e-03)x_7$ 

4) Model 64: 

$log(y^{hat})=(1.210e+01)-(1.258e-05)x_1+(1.382e-05)x_3+  (3.717e-05)x_5   -(6.015e-03 )x_7$

5) Model 99: 

$log(y^{hat})=(1.221e+01 )-(1.036e-05)x_1+( 1.358e-05)x_3+ (3.176e-05)x_5   -(4.477e-02)x_6-(5.876e-03)x_7$

6) Model 120: 

$log(y^{hat})=(1.232e+01) -(1.084e-05)x_1+ (1.484e-05)x_3-(2.332e-06)x_4+(3.223e-05)x_5-(7.125e-02)x_6-(6.046e-03)x_7$

7) Model 127:

$log(y^{hat})= (1.234e+01) -(1.073e-05)x_1 -(2.442e-04)x_2 +(1.481e-05)x_3  -(2.328e-06)x_4+ (3.206e-05)x_5 -(7.059e-02)x_6-(6.052e-03 )x_7$


\end{shaded}

```{r, echo = FALSE, out.height= "30%"}

model <- lm(log(y)~x1+x2+x3+x4+x5+x6+x7,data=reduce_dat)
#ols_step_all_possible(model)
plot(ols_step_all_possible(model))
```

# Evaluation of PRESS and VIF of candidate models:

\colorlet{shadecolor}{gray!10}

\begin{shaded}

Once we identified the "best"" candidate models, we compare its predicted residual error sum of squares (PRESS) statistic with other candidate models and selected the model with the smallest value. We also compare candidate models by performing a variance inflation factor (VIF) in order to quantify the severity of multicollinearity in the model.

*  The model with the lowest PRESS value is Model 64 however there is evidence of multicolliniearity.

*  The model with the second lowest PRESS value is Model 29  and the same model doesnt show any evidence of multicolinearity in the variance inflation factor test of each regressor.

\end{shaded}

### PRESS Statistic:

```{r, echo = FALSE}
fit1 <- lm(log(y)~x4,data=reduce_dat)
fit8 <- lm(log(y)~x3+x7,data=reduce_dat)
fit29 <- lm(log(y)~x3+x6+x7,data=reduce_dat)
fit64 <- lm(log(y)~x1+x3+x5+x7,data=reduce_dat)
fit99 <- lm(log(y)~x1+x3+x5+x6+x7,data=reduce_dat)
fit120 <- lm(log(y)~x1+x3+x4+x5+x6+x7,data=reduce_dat)
fit127 <- lm(log(y)~x1+x2+x3+x4+x5+x6+x7,data=reduce_dat)


paste("Model 1 PRESS:",round(PRESS(fit1),3))
paste("Model 8 PRESS:",round(PRESS(fit8),3))
paste("Model 29 PRESS:",round(PRESS(fit29),3))
paste("Model 64 PRESS:",round(PRESS(fit64),3))
paste("Model 99 PRESS:",round(PRESS(fit99),3))
paste("Model 120 PRESS:",round(PRESS(fit120),3))
paste("Model 127 PRESS:",round(PRESS(fit127),3))

```

### Multicolinearity Check:

```{r, echo =FALSE}
paste("Variance Inflation Factor")
vif(fit8)
vif(fit29)
vif(fit64)
vif(fit99)
vif(fit120)
vif(fit127)
```

# Residual Plots of best models:

```{r out.height= "25%", echo= FALSE}
p <- autoplot(fit29,size = 0.5, colour = 'sienna2')[1:2]
q <- autoplot(fit64,size = 0.5, colour = 'sienna2')[1:2]

grid.arrange(grobs = p@plots, ncol = 2, nrow = 1, labels = c("A", "B"),
             top= "Residual Plots for Model 29")
grid.arrange(grobs = q@plots, ncol = 2, nrow = 1, top= "Residual Plots for Model 64")
```



# Cross Validation:

\colorlet{shadecolor}{gray!10}

\begin{shaded}

After splitting the data into 4 groups and repeating the fold cross-validation by 12, the best model is Model 64:

$log(y^{hat}) = (1.210e+01) -(1.258e-05)Total.Population + (1.382e-05)Median.Household.Income + (3.717e-05)Total.Housing.Units -(6.015e-03)Housing.Affordability.Index$

\end{shaded}
\pagebreak

```{r, echo=FALSE}
m0 <- lm(y~x1+x3+x5+x7, data = reduce_dat)
m1 <- lm(y~x3+x6+x7, data = reduce_dat)

folds <- cvFolds(nrow(reduce_dat), K = 4, R =12) #type = "random", "consecutive", "interleaved"
cvfit_0 <- cvLm(m0, cost = rtmspe,folds = folds)
cvfit_1 <- cvLm(m1, cost = rtmspe,folds = folds)

cvFits <- cvSelect(LS0 = cvfit_0, LS1 = cvfit_1)


```
#  Exploratory Data Analysis:

\begin{shaded}

In houston most zip codes have a diversity index close to 80. However we see that neigborhoods with low diversity index tend to be situated in the extremes. The less diverse areas either have a very large per capita income and median home value, or a very per capita income and median home value. Also we see a positive relashionship between both variables.
\end{shaded}
```{r, echo= FALSE, out.height= "30%"}
#------------------------
library(GGally)
library(ggplot2)
library(RCurl)
library(bitops)



ggplot(aes(x = Per.Capita.Income, y = Median.Home.Value), data = reduce_dat)+
  geom_point(aes(colour = Diversity.Index1), position ='jitter',size = 0.5, shape=21)+
  scale_color_continuous(low = 'blue', high = 'red')

```
\begin{shaded}
Total population and median home value are not correlated. However we found some intesresting insights about how  Housing affordability index relates to both variables. The housing affordability index is large for houston areas with median home value bellow 250,000.00 dollars.
\end{shaded}

```{r, echo = FALSE, out.height= "30%"}
ggplot(aes(x = Total.Population, y = Median.Home.Value), data = reduce_dat)+ 
  geom_point(aes(colour = Housing.Affordability.Index2),position ='jitter',size = 0.5)+
  scale_color_continuous(low = 'blue', high = 'sienna2')


```
\begin{shaded}

Median Household income is positively correlated to Median home value. However, total housing units does not appear to have any linear relashion with median home value.

\end{shaded}
```{r, echo = FALSE, out.height= "30%"}

ggplot(aes(x = Median.Household.Income,y =Median.Home.Value), data = reduce_dat)+
  geom_point(aes(colour = Total.Housing.Units), shape = 21, size =1)
```

### Exploratory Analisys conclusion
\begin{shaded}

Although we are able to see how the variables used for our best models relate to median home value, we must call attention to the fact that our model predicts a transformed version of the median home value; Therefore, the relashionship may be diffent. Using correlation we can have some insight of how the predictors relate to the logged median home value.
\end{shaded}

```{r, echo = FALSE}
paste("Logged Median value vs total population",round(cor(log(y),x1),3))
paste("Logged Median value vs median household income",round(cor(log(y),x3),3)) # positive
paste("Logged Median value vs total housing units",round(cor(log(y),x5),3))
paste("Logged Median value vs housing affordability index",round(cor(log(y),x7),3))

```
\begin{shaded}
It turns out that we have the same results of colleration. Median household income is positively correlated with logged median home value, and housing affordability index is negativelly correlatted with median home value. As for total population and total housing units, these are not correlated with median home value. The coeficients of our model agrees with this observation. That is, with all other variables held constant an increase of median household income by 1 dollars results in an increase of (1.382e-05) for logged median home value. In the same manner, an increase of housing affordability index by one unit, results in a decrease of (6.015e-03) for logged median home value.

\end{shaded}


\pagebreak

# References

