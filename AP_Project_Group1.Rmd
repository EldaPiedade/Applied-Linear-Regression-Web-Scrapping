---
title: "Applied Regression Final Project"
author: "Elda Piedade, Thomas Oswald"
date: "May 4th, 2020"
header-includes:
- \usepackage{xcolor}
- \usepackage{framed}
output: 
  pdf_document: default
  html_document:
    df_print: paged
   
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



# Introduction


\colorlet{shadecolor}{gray!10}


\begin{shaded}

The objective of this project is to find the best linear regression model to predict the median home value for the houses in the Houston neighborhoods. Data was gathered from 96 zip codes in Houston by utilizing python web scrapping resources to collect data from the Texas Hometown Locator website (owned by HTL, Inc.). With the dataset extracted and cleaned, exploratory data analysis and statistical analysis were performed to understand the relationship between the median home value and other variables, such as diversity index, per capita income and median home income . Based on the analysis, data was modeled with linear regression. 


\end{shaded}

### Importance

\colorlet{shadecolor}{gray!10}


\begin{shaded}

1. With a good model for prediction and analysis, individuals in Houston will be able to understand how to price their homes for sale.

2. Understanding how the demographic factors relate to median home value is valuable social knowledge.

\end{shaded}
### Data 
\colorlet{shadecolor}{gray!10}


\begin{shaded}
Response variable : Median Home Value

Predictors:

* $x_1$ -Total Population

* $x_2$ - Diversity Index

* $x_3$ - Median Household Income

* $x_4$ - Per Capita Income

* $x_5$ - Total Housing Units

* $x_6$ - Average Household Size

* $x_7$ - Housing affordability Index

\end{shaded}

### Explained variables:

\colorlet{shadecolor}{gray!10}

\begin{shaded}
1. The Diversity Index is a scale of 0 to 100 that represents the
likelihood that two persons, chosen at random from the same area,
belong to different races or ethnic groups. If an area's entire
population belongs to one race and one ethnic group, then the area
has zero diversity. An area's diversity index increases to 100 when
the population is evenly divided into two or more race/ethnic
groups.

2. The Housing Affordability Index base is 100 and represents a
balance point where a resident with a median household income
can normally qualify to purchase a median price home. Values
above 100 indicate increased affordability, while values below
100 indicate decreased affordability.

3. The outcome variable is the median home value ( median price
home).

\end{shaded}

\pagebreak

# Data Loading & Checking full Model Accuracy:

\colorlet{shadecolor}{gray!10}

\begin{shaded}

After loading the data and creating a full Linear Regression (LR) model, we found that the model is not adequate. The residuals have a funnel and bowl shape; the residuals have a  heavily-tailed distribution, and the data has three possible influential points. To address this problem, we will inspect the data and perform a few transformations.

\end{shaded}

### First model - Residual Plots:
```{r, echo= FALSE, out.height= "27%"}
suppressMessages(library(ggfortify))
suppressMessages(library(car))
suppressMessages(library(olsrr))
suppressMessages(library(MPV))
suppressMessages(library(gridExtra))
suppressMessages(library(cvTools))

reduce_dat <- read.csv("CSV/DATA/texas_Gazetteer_numeric.csv")
reduce_dat <- reduce_dat[c("Median.Home.Value","Total.Population","Diversity.Index1","Median.Household.Income",
           "Per.Capita.Income","Total.Housing.Units", "Average.Household.Size","Housing.Affordability.Index2")]

y <- reduce_dat$Median.Home.Value
x1 <- reduce_dat$Total.Population
x2 <- reduce_dat$Diversity.Index1
x3 <- reduce_dat$Median.Household.Income
x4 <- reduce_dat$Per.Capita.Income
x5 <- reduce_dat$Total.Housing.Units
x6 <- reduce_dat$Average.Household.Size
x7 <- reduce_dat$Housing.Affordability.Index2

fit <- lm(y~x1+x2+x3+x4+x5+x6+x7,data=reduce_dat)
autoplot(fit, size = 0.5, colour = 'sienna2')[1:2]
```


# Data Transformation

\colorlet{shadecolor}{gray!10}

\begin{shaded}

The constant variance of error assumptions can often be solved with a response variable transformations. The log transformations performed the best compared to other transformations. It has the most appropriate properties for the normality of residuals and constant variance. The residual plot for the other transformation (reciprocal, square root, reciprocal square root, and inverse) did not show much improvement from the original model. In conclusion, our best transformation is the "log" transformation. The residual plot does not appear to have any alarming shape, and the residuals are normally distributed, except for the problematic observations, 9, 49, and 53. Our new transformed model indicates that a linear model provides a decent fit to the data.

\end{shaded}

### Log Transformed model - Residual Plots:

```{r, echo= FALSE,out.height= "27%"}
y_recs <- y^(-1/2)
y_rec <- y^(-1)
fit1 <- lm(sqrt(y)~x1+x2+x3+x4+x5+x6+x7,data=reduce_dat)
fit2 <- lm(log(y)~x1+x2+x3+x4+x5+x6+x7,data=reduce_dat)
fit3 <- lm(y_recs ~x1+x2+x3+x4+x5+x6+x7,data=reduce_dat)
fit4 <- lm(y_rec~x1+x2+x3+x4+x5+x6+x7,data=reduce_dat)

autoplot(fit2,size = 0.5, colour = 'sienna2')[1:2]
```


# Influencial Points :

\begin{shaded}

An observation with a large distance between itself and the center of the x-space as well as a large residual is likely to be influential. An influential point should only be discarded if there is an error in recording a measured value, the sample point is invalid or the observation is not part of the population that was intended to be sampled. An influential point should not be discarded if the point is a valid observation. 

Cook’s distance defines influence as a combination of leverage and residual size.A rule thumb is that an observation has high influence if Cook’s distance exceeds  4/(n-p -1) where n is the number of observations and p the number of predictor variables (Kassambara, et al.).

In our case, we have identified three influential points, observation 9, 49, and 92. These observations are above Cook's distance of 0.04545455.  

1. With regards to observation 9, it has an error with the Housing Affordability index of zero. This is highly unlikely because the affordability of zero would not allow people to buy houses in the area. We will delete this observation and continue evaluating possible models.

2. Concerning observation 49, it is located in northeast Houston. It has a population smaller than the surrounding zip codes and has a median income lower than 20K. Part of this zip code is industrial and undeveloped which explains the lower population and median income.

3. With regards to observation 92, it is located in the energy corridor of Houston, off I-10 West, the population is significantly less than the neighboring zip codes and has a median housing income greater than 165K. Half of this zip code included the George Bush Park which explains the lower population and median income.


\end{shaded}
```{r echo= FALSE,out.height= "26%"}
autoplot(fit2,4,size = 0.5, colour = 'sienna2')
```


# Full Regression Model Significance Test

\colorlet{shadecolor}{gray!10}

\begin{shaded}

Our linear regression model is significant given that the p-value for the F-test is smaller than our level of significance 0.05. This means that at least one regressor has a linear relationship with the median home value. With the marginal t-test for any individual regressor coefficient, we found that the intercept, diversity index, median household income, per capita income, and housing affordability index are significant for predicting log(median home value) - that is, there is evidence that these coefficients are significantly not zero. 

** Deleting observation nine changed the significance of each predictor. Before deleting this point, diversity index and per capita income were deemed insignificant. **

In more detail the F-test Hypothesis is :

$H_0$: All coefficients are significantly 0.

$H_1$:  At least one coefficient is a significant predictor.

The  t-test Hypothesis is :

$H_0$: $B_j$ = 0

$H_1$:  $B_j$ != 0 

This is a good step in our analysis and important to keep in mind.

\end{shaded}

```{r, echo= FALSE}
## Update dataset after removing the 9th obs.
reduce_dat = subset(reduce_dat, reduce_dat$Housing.Affordability.Index2 != 0)
y <- reduce_dat$Median.Home.Value
x1 <- reduce_dat$Total.Population
x2 <- reduce_dat$Diversity.Index1
x3 <- reduce_dat$Median.Household.Income
x4 <- reduce_dat$Per.Capita.Income
x5 <- reduce_dat$Total.Housing.Units
x6 <- reduce_dat$Average.Household.Size
x7 <- reduce_dat$Housing.Affordability.Index2

summary(lm(log(Median.Home.Value) ~., data = reduce_dat))
```

# Evaluating all possible subset regression models

\colorlet{shadecolor}{gray!10}

\begin{shaded}

In looking for the "best" model, certain criteria must be met for proper variable selection of the regressor equation.  These criteria help us to be able to explain the data easily with redundant predictors removed to minimize cost and to avoid multicollinearity in our regression model. 

The criteria for our variable selection include:

1) Large $R^2$ value

2) Maximum Adjusted $R^2$ value

3) Minimum MSres

4) Minimum Mallow's $C_p$ Statistic value.


Based on the above criteria, the "best" candidate models are:

1) Model 1: $log(y^{hat})= B_0 + B_4x_4$

2) Model 8:	$log(y^{hat})= B_0 +  B_3x_3 + B_7x_7$

3) Model 29: $log(y^{hat})= B_0 + B_3x_3 + B_4x_4 + B_7x_7$ 

4) Model 64: $log(y^{hat})= B_0 +  B_2x_2 + B_3x_3 + B_4x_4 + B_7x_7$

5) Model 99: $log(y^{hat})= B_0 + B_2x_2 + B_3x_3 + B_4x_4 + B_6x_6 + B_7x_7$

6) Model 120: $log(y^{hat})=B_0 + B_2x_2 + B_3x_3 + B_4x_4 + B_5x_5 + B_6x_6 + B_7x_7$

7) Model 127: $log(y^{hat})= B_0 + B_1x_1 + B_2x_2 + B_3x_3 + B_4x_4 + B_5x_5 + B_6x_6 + B_7x_7$


\end{shaded}

```{r, echo = FALSE, out.height= "30%"}

model <- lm(log(y)~x1+x2+x3+x4+x5+x6+x7,data=reduce_dat)
#ols_step_all_possible(model)
plot(ols_step_all_possible(model))
```


# Evaluation of PRESS and VIF of candidate models:

\colorlet{shadecolor}{gray!10}

\begin{shaded}

Once we identified the "best" candidate models, we compared their predicted residual error sum of squares (PRESS) statistic with other candidate models and selected the model with the smallest value. We also compare candidate models by performing a variance inflation factor (VIF) to quantify the severity of multicollinearity in the model. Multicollinearity refers to a situation when two or more explanatory variables in a multiple regression model are highly linearly related. Indicators of multicollinearity can be present in a model with inflated coefficient estimates. VIF values larger than 10 are and an indicator that multicollinearity is present.

*  The model with the lowest PRESS value is Model 64. No multicollinearity is found.

*  The model with the second-lowest PRESS value is Model 99. However, this model has multicollinearity which is a property that can impact the ability to estimate the regression coefficients. For this reason, we selected Model 29 as our second-best model.

* Model 29 has the third-lowest PRESS value and has no multicollinearity problem.

* Also, the normal probability plot of the residuals in Model 29 has a more normally distributed appearance.

\end{shaded}

### PRESS Statistic:

```{r, echo = FALSE}
fit1 <- lm(log(y)~x4,data=reduce_dat)
fit8 <- lm(log(y)~x3+x7,data=reduce_dat)
fit29 <- lm(log(y)~x3+x4+x7,data=reduce_dat)
fit64 <- lm(log(y)~x2+x3+x4+x7,data=reduce_dat)
fit99 <- lm(log(y)~x2+x3+x4+x6+x7,data=reduce_dat)
fit120 <- lm(log(y)~x2+x3+x4+x5+x6+x7,data=reduce_dat)
fit127 <- lm(log(y)~x1+x2+x3+x4+x5+x6+x7,data=reduce_dat)


paste("Model 1 PRESS:",round(PRESS(fit1),3))
paste("Model 8 PRESS:",round(PRESS(fit8),3))
paste("Model 29 PRESS:",round(PRESS(fit29),3))
paste("Model 64 PRESS:",round(PRESS(fit64),3))
paste("Model 99 PRESS:",round(PRESS(fit99),3))
paste("Model 120 PRESS:",round(PRESS(fit120),3))
paste("Model 127 PRESS:",round(PRESS(fit127),3))

```

### Multicolinearity Check:

```{r, echo =FALSE}
paste("Variance Inflation Factor")
vif(fit8)
vif(fit29)
vif(fit64)
vif(fit99)
vif(fit120)
vif(fit127)
```

# Residual Plots of best models:

```{r out.height= "25%", echo= FALSE}
p <- autoplot(fit29,size = 0.5, colour = 'sienna2')[1:2]
q <- autoplot(fit64,size = 0.5, colour = 'sienna2')[1:2]

grid.arrange(grobs = p@plots, ncol = 2, nrow = 1, labels = c("A", "B"),
             top= "Residual Plots for Model 29")
grid.arrange(grobs = q@plots, ncol = 2, nrow = 1, top= "Residual Plots for Model 64")
```

# Cross Validation:

\colorlet{shadecolor}{gray!10}

\begin{shaded}

The process of cross-validation is the splitting of data into parts for estimation and prediction. After the data is randomly shuffled, the splitting into groups is performed. The process is repeated with the desired iterations and the average cross-validation value is returned. According to Cross-Validation, after splitting our data into 4 groups, with K = 4, and repeating the fold cross-validation with 12 iterations, the best model is Model 64. 

Considering the $R^2_p$ statistic, we decided that the best model is Model 64 because it has the largest $R^2_p$ compared to Model 29. Although as we increase regressors the $R^2_p$ inflates, the cross-validation supports the evidence that the best model is Model 64. 

Model 64:
```{r, echo = FALSE, include= FALSE}
fit64
```

$log(y^{hat}) = ( 1.176e+01) + (4.860e-03)Diversity Index + (8.682e-06) Median Home Value + (1.181e-05) Per Capita Income  -  (6.254e-03) Housing Affordability Index$

\end{shaded}

```{r, echo=FALSE}
new_y = log(y)
m0 <- lm( new_y  ~ x2+x3+x4+x7, data = reduce_dat)
m1 <- lm( new_y  ~ x3+x4+x7, data = reduce_dat)

folds <- cvFolds(nrow(reduce_dat), K = 4, R =12)

cvfit_0 <- cvLm(m0, cost = rtmspe,folds = folds)
cvfit_1 <- cvLm(m1, cost = rtmspe,folds = folds)

cvFits <- cvSelect(LS0 = cvfit_0, LS1 = cvfit_1)
cvFits
paste("Model 64 R^2_p")
anova0 = anova(m0)
sst0 = sum(anova0$'Sum Sq') #Calculate the total sum of squares
library(MPV)
1-PRESS(m0)/(sst0)   # Calculate the predictive R^2
paste("Model 29 R^2_p")
anova1 = anova(m1)
sst1 = sum(anova1$'Sum Sq') #Calculate the total sum of squares
library(MPV)
1-PRESS(m1)/(sst1)   # Calculate the predictive R^2
```

# Confidence Interval for coefficients:

In practice, it is important to understand how strong our estimated coefficients are. We have computed the 95% confidence interval for coefficients in the best model. They are presented as:

```{r, echo =FALSE}
result_1 = summary(m0)

lower = c(result_1$coef[2,1] - qt(0.975, df = result_1$df[2]) * result_1$coef[2, 2],
          result_1$coef[3,1] - qt(0.975, df = result_1$df[2]) * result_1$coef[3, 2],
          result_1$coef[4,1] - qt(0.975, df = result_1$df[2]) * result_1$coef[4, 2],
          result_1$coef[5,1] - qt(0.975, df = result_1$df[2]) * result_1$coef[5, 2])

upper = c(result_1$coef[2,1] + qt(0.975, df = result_1$df[2]) * result_1$coef[2, 2],
          result_1$coef[3,1] + qt(0.975, df = result_1$df[2]) * result_1$coef[3, 2],
          result_1$coef[4,1] + qt(0.975, df = result_1$df[2]) * result_1$coef[4, 2],
          result_1$coef[5,1] + qt(0.975, df = result_1$df[2]) * result_1$coef[5, 2])

names = c("B2","B3","B4","B7")
cbind("Coef" = names, "lower" = lower,
  "upper" = upper)

```

#  Exploratory Data Analysis:

\begin{shaded}

In Houston, most zip codes have a diversity index close to 80. However, we see that neighborhoods with low diversity index tend to be situated in the extremes. The less diverse areas either have a very large per capita income and median home value, or a very small per capita income and median home value. Also, we see a positive relationship between both variables.

\end{shaded}
```{r, echo= FALSE, out.height= "23%"}
library(GGally)
library(ggplot2)
library(RCurl)
library(bitops)



ggplot(aes(x = Per.Capita.Income, y = Median.Home.Value), data = reduce_dat)+
  geom_point(aes(colour = Diversity.Index1), position ='jitter',size = 0.5, shape=21)+
  scale_color_continuous(low = 'blue', high = 'red')

```
\begin{shaded}
The total population and median home value are not correlated. However, we found some interesting insights into how the Housing affordability index relates to both variables. The housing affordability index is large for Houston areas with a median home value below 250,000.00 dollars. The total population has virtually no effect on housing affordability index or median home value.
\end{shaded}

```{r, echo = FALSE, out.height= "30%"}
ggplot(aes(x = Total.Population, y = Median.Home.Value), data = reduce_dat)+ 
  geom_point(aes(colour = Housing.Affordability.Index2),position ='jitter',size = 0.5)+
  scale_color_continuous(low = 'blue', high = 'sienna2')


```
\begin{shaded}

Median Household income is positively correlated to the Median home value. As the Median home value and Median Household income decreases, the Average Household Size increases. This means that less wealthy families tend to have on average more people residing in their homes.

\end{shaded}
```{r, echo = FALSE, out.height= "30%"}

ggplot(aes(x = Median.Household.Income,y =Median.Home.Value), data = reduce_dat)+
  geom_point(aes(colour = Average.Household.Size), shape = 21, size =1)
```

### Exploratory Analisys conclusion
\begin{shaded}

Although we can see how the variables used for our best models relate to median home value, we must call attention to the fact that our model predicts a transformed version of the median home value. Therefore, the relationship may be different. Using correlation, we can have some insight into how the predictors relate to the logged median home value.

\end{shaded}

```{r, echo = FALSE}
paste("Logged Median value vs Diversity Index ",round(cor(log(y),x2),3))
paste("Logged Median value vs median household income",round(cor(log(y),x3),3)) # positive
paste("Logged Median value vs Per capita Income ",round(cor(log(y),x4),3))
paste("Logged Median value vs housing affordability index",round(cor(log(y),x7),3))

```
\begin{shaded}

It turns out that the correlation of almost all predictors with the logged median home value coincides with the effects of the coefficients in our best model. Median household income and Per capita income are positively correlated with logged median home value, and these predictors have positive coefficients in the model. The housing affordability index is negatively correlated with median home value and has a negative coefficient in the model. However, the diversity index is negatively correlated with the median home value but has a positive coefficient in the model.

In more detailed:

* With all regressors held fixed, an increase of median household income by 1 dollar is associated with an increase of logged median home value by (8.682e-06) units on average.

* With all regressors held fixed an increase of per capita income by 1 dollar is associated with an increase of logged median home value by (1.181e-05) units on average. 

* With all regressors held fixed an increase of diversity index by 1 unit is associated with an increase of logged median home value by (4.860e-03) units on average. 

* With all regressors held fixed an increase of housing affordability index by 1 unit is associated with a decrease of logged median home value by (6.254e-03) units on average. 


\end{shaded}

# Conclusion

\begin{shaded}

Our objective was to find the best linear regression model to predict the median home value of houses in the Houston neighborhoods.  With the use of diagnostics, Data Transformation, and variable selection, we were able to create a subset regression model from the data collected from the Texas Gazetteer.  Our best model is based on the logged response variable of Median Home Value, with Median Home Income, Per Capita Income, Diversity Index, and Housing Affordability Index as the regressors. Furthermore, we performed exploratory data analysis to better inform ourselves about the data. Furthermore, by identifying influential points we were able to identify an error within our data. In consequence, we were able to improve the performance of our model. When the regression was done including observation 9 we obtained larger PRESS statistics and the significance of the predictors was not in accordance with what we found by visualizing the data. When we modeled our data without observation 9, the PRESS values were smaller and the significance of the predictors resulted to be in accordance with our exploratory data analysis.

\end{shaded}

\pagebreak

# References

Montgomery, Douglas C., and Anne G. Ryan. Introduction to Linear Regression Analysis, Fifth Edition. Wiley, 2013.
     
Kassambara, et al. “Linear Regression Assumptions and Diagnostics in R: Essentials.” STHDA, 11 Mar. 2018, www.sthda.com/english/articles/39-regression-model-diagnostics/161-linear-regression-assumptions-and-diagnostics-in-r-essentials/.

“ZIP Code 77050.” ZIP Code 77050 Map, Demographics, More for Houston, TX,    www.unitedstateszipcodes.org/77050/.

“ZIP Code 77094.” ZIP Code 77094 Map, Demographics, More for Houston, TX, www.unitedstateszipcodes.org/77094/.

“Houston County TX Data .” Houston County TX Data &amp; Peer Group Rankings, texas.hometownlocator.com/tx/houston/.

“Texas Gazetteer.” Maps, Data, Photos for 4,743 Locations, texas.hometownlocator.com/.